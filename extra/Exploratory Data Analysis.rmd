---
title: "Exploratory Data Analysis"
author: "Callum Stewart"
output: github_document
---
```{r libraries, message=FALSE}
library(tidyverse)
library(readr)
library(dplyr)
library(skimr)
library(here)
library(forcats)
library(tidymodels)
```

```{r read_data, message=FALSE}
f1merged <- read_csv(here("data/f1merged.csv"))
f1merged_hybrid <- read_csv(here("data/f1merged_hybrid.csv"))
stops_merged <- read_csv(here("data/stops_merged.csv"))
```

```{r hybrid_era}
hybrid_era <- (2014:2020)
```

```{r mercedes_success}
team_colours <- c("Mercedes" = "#00d2be",
                  "Red Bull" = "#0600ef",
                  "Ferrari" = "#dc0000",
                  "Racing Point" = "#F596C8",
                  "Force India" = "#f596c8",
                  "AlphaTauri" = "#ffffff",
                  "McLaren" = "#ff8700",
                  "Renault" = "#fff500",
                  "Williams" = "#0082fa",
                  "Toro Rosso" = "#469BFF",
                  "Lotus F1" = "#000000",
                  "Alfa Romeo" = "#960000",
                  "Sauber" = "#960000",
                  "Haas F1 Team" = "#787878")
                  


f1merged_hybrid %>%
  filter(positionText == 1) %>%
count(constructorname, sort = TRUE) %>% 
  ggplot(aes(x = n, 
             y = factor(constructorname, levels = rev(levels(factor(constructorname)))),
             fill = constructorname)) +
  geom_col(aes()) + 
 scale_fill_manual(values = team_colours) +
  labs(title = "Race Wins by Constructor",
       subtitle = "In the hybrid era (2014-2020)",
       x = "Race Wins",
      y = "Constructor") +
  guides(fill = "none")
  

f1merged_hybrid %>%
  filter(positionText %in% 1:3) %>%
count(constructorname, sort = TRUE) %>% 
  ggplot(aes(x = n, 
             y = factor(constructorname, levels = rev(levels(factor(constructorname)))),
             fill = constructorname)) +
  geom_col(aes()) + 
 scale_fill_manual(values = team_colours) +
  labs(title = "Podium Finishes by Constructor",
       subtitle = "In the hybrid era (2014-2020)",
       x = "Podiums",
      y = "Constructor") + 
  guides(fill = "none")
  
f1merged_hybrid %>%
  filter(grid == 1) %>%
count(constructorname, sort = TRUE) %>% 
  ggplot(aes(x = n, 
             y = factor(constructorname, levels = rev(levels(factor(constructorname)))),
             fill = constructorname)) +
  geom_col(aes()) + 
 scale_fill_manual(values = team_colours) +
  labs(title = "Pole Positions by Constructor",
       subtitle = "In the hybrid era (2014-2020)",
       x = "Pole Positions",
      y = "Constructor") + 
  guides(fill = "none")

f1merged_hybrid %>%
  group_by(constructorname) %>%
  summarise(total_points = sum(points)) %>%
  filter(total_points > 5) %>%
   ggplot(aes(x = total_points, 
             y = factor(constructorname, levels = rev(levels(factor(constructorname)))),
             fill = constructorname)) +
  geom_col(aes()) + 
 scale_fill_manual(values = team_colours) +
  labs(title = "Total Championship Points by Constructor",
       subtitle = "In the hybrid era (2014-2020)",
       x = "Points",
      y = "Constructor") + 
  guides(fill = "none")

```


```{r summary_stats, echo = FALSE, eval = FALSE}

constructor_stats <- function(constructor) {

f1merged_hybrid %>%
  group_by(constructorname) %>%
  filter(constructorname %in% key_teams) %>%
  summarise(mean_grid_pos = mean(grid),
          mean_finish_pos = mean(position, na.rm = TRUE),
             mean_fl_rank = mean(rank, na.rm = TRUE),
               med_points = median(points),
              mean_points = (sum(points))/(n_distinct(f1merged_hybrid$raceId)))
}
            

constructor_stats("Mercedes")
constructor_stats("Ferrari")
constructor_stats("Red Bull")
constructor_stats("McLaren")
constructor_stats("Williams")
```

```{r quali_vs_race}
f1merged_hybrid %>%
  filter(!is.na(position)) %>%
  ggplot(aes(x = grid, y = position)) +
  geom_jitter() +
  geom_smooth(method = lm,
              formula = y ~ x,
              colour = "red") +
  labs(x = "Qualifying Position",
       y = "Race Finishing Position",
       title = "Qualifying Position vs. Finishing Position",
       subtitle = "In the hybrid era (2014-2020)")

```


```{r retirements}
key_teams <- c("Ferrari", 
               "McLaren",
               "Mercedes",
               "Red Bull",
               "Williams")


f1merged_hybrid %>%
  group_by(constructorname) %>%
  filter(constructorname %in% key_teams & positionText == "R") %>%
  count(constructorname, sort = TRUE) %>%
  summarise(mean_ret_per_season = n/(n_distinct(f1merged_hybrid$year))) %>%
  ggplot(aes(x = mean_ret_per_season, 
             y = factor(constructorname, levels = rev(levels(factor(constructorname)))), 
             fill = constructorname)) +
  geom_col() + 
  scale_fill_manual(values = team_colours) +
  labs(x = "Mean Retirements Per Season",
       y = "Constructor",
       title = "Retirements Per Season by Constructor",
       subtitle = "In the Hybrid Era (2014-2020)") +
  guides(fill = "none")
```
```{r grid_pos_0?}
#looking at outliers
f1merged_hybrid %>%
  filter(grid == 0)
#Seem to be a small number of irregularities we can safely remove
```

```{r qualifying_model}
quali_grid_tidy <- f1merged_hybrid %>%
  filter(!is.na(position) & grid != 0) 
 
  
  quali_grid_tidy %>%
   ggplot(aes(x = grid, y = position)) +
  geom_jitter() +
  geom_smooth(method = lm,
              formula = y ~ x,
              colour = "red") +
  labs(x = "Qualifying Position",
       y = "Race Finishing Position",
       title = "Qualifying Position vs. Finishing Position",
       subtitle = "In the hybrid era (2014-2020)") 

posi_grid_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(position ~ grid, data = quali_grid_tidy)

tidy(posi_grid_fit)

glance(posi_grid_fit)
```


```{r fitted_vs_residuals}
posi_grid_fit_aug <- augment(posi_grid_fit$fit)

posi_grid_fit_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_jitter() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  labs(x = "Predicted Value",
       y = "Residuals",
       title = "Predicted Values vs Residuals")


```

```{r quali_vs_grid_leaders}
quali_grid_tidy_leaders <- quali_grid_tidy %>%
  filter(grid <=5)
 
  quali_grid_tidy_leaders %>%
   ggplot(aes(x = grid, y = position)) +
  geom_jitter() +
  geom_smooth(method = lm,
              formula = y ~ x,
              colour = "red") +
  labs(x = "Qualifying Position",
       y = "Race Finishing Position",
       title = "Qualifying Position vs. Finishing Position (Top 5 Qualifiers)",
       subtitle = "In the hybrid era (2014-2020)") 

posi_grid_fit_leaders <- linear_reg() %>%
  set_engine("lm") %>%
  fit(position ~ grid, data = quali_grid_tidy_leaders)

tidy(posi_grid_fit_leaders)

glance(posi_grid_fit_leaders)


posi_grid_fit_leaders_aug <- augment(posi_grid_fit_leaders$fit)

posi_grid_fit_leaders_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_jitter() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  labs(x = "Predicted Value",
       y = "Residuals",
       title = "Predicted Values vs Residuals")
```


```{r quali_vs_grid_rest}

quali_grid_tidy_rest <- quali_grid_tidy %>%
  filter(grid >5)
 
  quali_grid_tidy_rest %>%
   ggplot(aes(x = grid, y = position)) +
  geom_jitter() +
  geom_smooth(method = lm,
              formula = y ~ x,
              colour = "red") +
  labs(x = "Qualifying Position",
       y = "Race Finishing Position",
       title = "Qualifying Position vs. Finishing Position (Without Top 5 Qualifiers)",
       subtitle = "In the hybrid era (2014-2020)") 

posi_grid_fit_rest <- linear_reg() %>%
  set_engine("lm") %>%
  fit(position ~ grid, data = quali_grid_tidy_rest)

tidy(posi_grid_fit_rest)

glance(posi_grid_fit_rest)


posi_grid_fit_rest_aug <- augment(posi_grid_fit_rest$fit)

posi_grid_fit_rest_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_jitter() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  labs(x = "Predicted Value",
       y = "Residuals",
       title = "Predicted Values vs Residuals")

```

Models suggest a much stronger correlation between grid position and finishing position for the drivers qualifying in the top 5. 

If you qualify near the front you are likely to stay there, not as strong a relationship further back in the field. 

If you have one of the fastest cars and qualify near the front you are likely to leave the rest of the field behind (better pace, not as likely to be in someone's dirty air) (also less likely to get involved in any incidents/collisions, particularly at race start.) (All of these factors would be strongest if you qualified P1)

Qualifying near the front of the grid is a very strong predictor of race success. But of course having the clear fastest car would also correlate very strongly with both of these.

```{r mercedes_vs_the_world_poles}
f1merged_hybrid %>%
  mutate(mercedes_or_not = if_else
         (constructorname == "Mercedes",
                     true = "Mercedes",
                    false = "Everyone Else")) %>%
  filter(grid == 1) %>%
count(mercedes_or_not, sort = TRUE) %>% 
  ggplot(aes(x = n, 
             y = factor(mercedes_or_not, levels = rev(levels(factor(mercedes_or_not)))),
             fill = mercedes_or_not)) +
  geom_col(aes()) + 
 scale_fill_manual(values = team_colours) +
  labs(title = "Pole Positions by Constructor",
       subtitle = "In the hybrid era (2014-2020)",
       x = "Pole Positions",
      y = "Constructor") + 
  guides(fill = "none")




```
```{r mercedes_vs_the_world_stats} 
f1merged_hybrid %>%
  filter(position == 1) %>%
  count(constructorname) %>%
  rename(race_wins = n) %>%
  mutate(percent = (race_wins / sum(race_wins)) * 100)

f1merged_hybrid %>%
  filter(grid == 1) %>%
  count(constructorname) %>%
  rename(pole_positions = n) %>%
  mutate(percent = (pole_positions / sum(pole_positions)) * 100)

```

```{r mercedes_vs_the_world_wins}
f1merged_hybrid %>%
  mutate(mercedes_or_not = if_else
         (constructorname == "Mercedes",
                     true = "Mercedes",
                    false = "Everyone Else")) %>%
  filter(position == 1) %>%
count(mercedes_or_not, sort = TRUE) %>% 
  ggplot(aes(x = n, 
             y = factor(mercedes_or_not, levels = rev(levels(factor(mercedes_or_not)))),
             fill = mercedes_or_not)) +
  geom_col(aes()) + 
 scale_fill_manual(values = team_colours) +
  labs(title = "Race Wins by Constructor",
       subtitle = "In the hybrid era (2014-2020)",
       x = "Race Wins",
      y = "Constructor") + 
  guides(fill = "none")
```
```{r pole_win_percent}
f1merged_hybrid %>% 
  filter(grid == 1) %>%
 mutate(pole_win = if_else(!is.na(position) & position == 1,
              true = "Pole Sitter Wins",
             false = "Other Driver Wins")) %>%
  count(pole_win) %>%
  mutate(percentage = (n / sum(n)) * 100)

f1merged_hybrid %>% 
  filter(grid == 1) %>%
 mutate(pole_win = if_else(!is.na(position) & position %in% 1:3,
              true = "Pole Sitter Podium",
             false = "Pole Sitter Off Podium")) %>%
  count(pole_win) %>%
  mutate(percentage = (n / sum(n)) * 100)
  
  
```
 The driver who qualifies in 1st place goes on to win the race ~53% of the time, and finishes on the podium ~83% of the time.
 

```{r retirements_season}
ret_season <- f1merged_hybrid %>%
  filter(constructorname %in% key_teams & positionText == "R") %>%
  group_by(year, constructorname) %>%
  count(constructorname) %>%
  summarise(retirements = n, .groups = "drop")



ret_season
```


```{r points_season}
points_season <- f1merged_hybrid %>%
  filter(constructorname %in% key_teams) %>%
  group_by(year, constructorname) %>%
  summarise(total_points = sum(points), .groups = "drop")

points_season
 

```

```{r combined_rets_points}

rets_points_season <- inner_join(points_season, ret_season)

rets_points_season %>%
  group_by(constructorname) %>%
   summarise(mean_ret_season = sum(retirements) / (n_distinct(rets_points_season$year)),
             mean_points_season = sum(total_points) / (n_distinct(rets_points_season$year)))
```

```{r rets_points_plot}
key_team_colours <- c("Mercedes" = "#00d2be",
                  "Red Bull" = "#0600ef",
                  "Ferrari" = "#dc0000",
                  "McLaren" = "#ff8700",
                  "Williams" = "#0082fa")

rets_points_season %>%
  ggplot(aes(x = retirements, y = total_points, colour = constructorname)) +
  geom_point(size = 2.5) + 
  geom_smooth(method = "lm",
              formula = y ~ x,
              colour = "black",
              se = FALSE) +
  labs(x = "Retirements",
       y = "Total Championship Points",
       title = "Retirements vs Total Points",
       subtitle = "Per Season",
       colour = "Constructor") +
  scale_colour_manual(values = key_team_colours)

```


```{r rets_points_model}
rets_points_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(total_points ~ retirements, data = rets_points_season)

tidy(rets_points_fit)

glance(rets_points_fit)


rets_points_fit_aug <- augment(rets_points_fit$fit)

rets_points_fit_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_jitter() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  labs(x = "Predicted Value",
       y = "Residuals",
       title = "Predicted Values vs Residuals")
```
Our model predicts a constructor with 0 retirements in a season would score 515 points in a season (Dubious, probably due to our comparison group being unintentionally weighted towards top teams). It predicts that for each additional retirement in a season, a team would score 34 fewer points.

The R^2 value suggests 21.8% of the variability in points per season can be explained by the number of retirements in a season.

The predicted value vs. residuals plot doesn't show a clear pattern and appears to be a reasonably random scatter. As such a linear model is likely a good fit for our data.


```{r mean_pit_stops}
stops_merged %>%
  filter(constructorname %in% key_teams & !is.na(milliseconds)
        & milliseconds < 9*10^5) %>%
  group_by(constructorname) %>%
  summarise(mean_stop = mean(milliseconds),
            median_stop = median(milliseconds))
```

```{r stops_points}
#skewed version
stops_merged %>%
 filter(constructorname %in% key_teams & !is.na(milliseconds)) %>%
  ggplot(aes(x = milliseconds, 
             y = factor(constructorname, levels = rev(levels(factor(constructorname)))), 
             fill = constructorname)) +
  geom_boxplot() +
  scale_fill_manual(values = key_team_colours) +
  scale_x_continuous(labels = label_number(scale = 0.001)) +
  guides(fill = "none") +
  labs(x = "Pit Stop Time (Seconds)",
       y = "Constructor",
       title = "Pit Stop Times by Constructor",
       caption = "A pit stop is recorded here as the total time in the pit lane")

# red flag stops removed
stops_merged %>%
 filter(constructorname %in% key_teams & !is.na(milliseconds)
        & milliseconds < 9*10^5) %>%
  ggplot(aes(x = milliseconds,
             y = factor(constructorname, levels = rev(levels(factor(constructorname)))), 
             fill = constructorname)) +
  geom_boxplot() +
  scale_fill_manual(values = key_team_colours) +
  scale_x_continuous(labels = label_number(scale = 0.001)) +
  guides(fill = "none") +
  labs(x = "Pit Stop Time (Seconds)",
       y = "Constructor",
       title = "Pit Stop Times by Constructor",
       caption = "A pit stop is recorded here as the total time in the pit lane")


#very suspicious outlying times
 stops_merged %>%
filter(constructorname %in% key_teams & !is.na(milliseconds)) %>%
   arrange(desc(milliseconds)) %>%
   relocate(year, racename, driverId, duration, milliseconds)
 
```
 - Note: Due to what is likely an error in data collection, a small number of red flagged (suspended for safety reasons) sessions have been recorded as pit stops. 

 - According to the data above the 2016 Brazilian Grand Prix saw 9 concurrent **33 minute** pit stops. In reality the race was red flagged after a driver crashed on the pit straight and there was a roughly 35 minute delay before it resumed.

 - The clearly erroneous pit stops are all over 15 minutes in duration, after which there is a large jump to the first genuine long stop at 1:49 in length. 
 As such it makes sense to remove any pit stops over 15 minutes duration from our analysis
 
```{r med_stops_season}
stops_season <- stops_merged %>%
  group_by(year, constructorname) %>%
  filter(constructorname %in% key_teams & !is.na(milliseconds)
        & milliseconds < 9*10^5) %>%
  summarise(median_stoptime = median(milliseconds))

stops_points_season <- inner_join(points_season, stops_season)

stops_points_season

```

```{r plot_points_stops_season}
stops_points_season %>%
  ggplot(aes(x = median_stoptime, y = total_points, colour = constructorname)) +
  geom_point(size = 2.5) + 
  geom_smooth(method = "lm",
              formula = y ~ x,
              colour = "black",
              se = FALSE) +
  labs(x = "Median Pit Stop Time (Seconds)",
       y = "Total Championship Points",
       title = "Pit Stop Time vs Total Points",
       subtitle = "Per Season",
       colour = "Constructor") +
  scale_colour_manual(values = key_team_colours) +
  scale_x_continuous(labels = label_number(scale = 0.001))
  
```

```{r model_points_stops_season}
stops_points_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(total_points ~ median_stoptime, data = stops_points_season)

tidy(stops_points_fit)

glance(stops_points_fit)


stops_points_fit_aug <- augment(stops_points_fit$fit)

stops_points_fit_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_jitter() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  labs(x = "Predicted Value",
       y = "Residuals",
       title = "Predicted Values vs Residuals")

```
Our model suggests that a team with a median stop time of 0 milliseconds would be expected to score 5759 points on average. In practice neither of those things are possible.

Our model suggests that all else held equal, for each 1 millisecond increase in a team's median stop time, they would be expected to score 0.2299 fewer points across a season.

The R^2 value of our model suggests that 19.6% of the variability in points per season can be explained by median stop time.

The plot of predicted values vs residuals is a random scatter with no pattern, suggesting that a linear model would be a good fit in this case.

```{r}
stops_rets_points <- inner_join(rets_points_season, stops_points_season)

stops_rets_points_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(total_points ~ median_stoptime + retirements, data = stops_rets_points)

tidy(stops_rets_points_fit)

glance(stops_rets_points_fit)


stops_rets_points_fit_aug <- augment(stops_rets_points_fit$fit)

stops_rets_points_fit_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_jitter() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  labs(x = "Predicted Value",
       y = "Residuals",
       title = "Predicted Values vs Residuals")

```
The combined model predicts that a team with 0 retirements in a season and a median stop time of 0 milliseconds would be expected to score 6202 points, which doesn't make any sense here.

Our model suggests that all else held equal, for each 1 millisecond increase in a team's median stop time, they would be expected to score 0.24 fewer points across a season.

Our model predicts that for each additional retirement in a season, a team would be expected to score 35 points fewer on average.

The combined model has an adjusted R^2 of 39.7, which suggests 38.7% of the variability in total points per season can be explained by median stop time *and* number of retirements.

The predicted values vs residuals plot again shows a random scatter, so a linear model is a good fit for this data.

```{r median_quali_season}
quali_season <- f1merged_hybrid %>%
  filter(constructorname %in% key_teams & !is.na(grid)) %>%
  group_by(year, constructorname) %>%
  summarise(median_grid = median(grid))

final_model <- inner_join(quali_season, stops_rets_points)

final_model_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(total_points ~ median_grid+ median_stoptime + retirements,
      data = final_model)

tidy(final_model_fit)

glance(final_model_fit)


final_model_fit_aug <- augment(final_model_fit$fit)

final_model_fit_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_jitter() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  labs(x = "Predicted Value",
       y = "Residuals",
       title = "Predicted Values vs Residuals")

```
Here I've added a further predictor variable to the model, median qualifying position per season.

This model has a higher adjusted R^2 (84.7%) than the previous 2 predictor variable model, but the plot of predicted values vs residuals seems to show a v shaped pattern and as such a linear model may not be suitable here.
